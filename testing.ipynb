{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from dictionary_learning import AutoEncoder\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_stats(model_str, layer, component, dset, seq_len=64, batch_size=5, simulate=False):\n",
    "\n",
    "    model = LanguageModel(model_str, device_map='cuda', dispatch=True)\n",
    "    if model_str == 'gpt2':\n",
    "        # Load GPT2 model\n",
    "\n",
    "        # load SAE\n",
    "        if component == 'resid':\n",
    "            n_feats = '32k'\n",
    "            loc = 'post'\n",
    "            ext = '.pt'\n",
    "        else:\n",
    "            n_feats = '128k'\n",
    "            loc = 'out'\n",
    "            ext = ''\n",
    "\n",
    "\n",
    "        repo = f\"jbloom/GPT2-Small-OAI-v5-{n_feats}-{component}-{loc}-SAEs\"\n",
    "        sae = AutoEncoder.from_hf(repo, f\"v5_{n_feats}_layer_{layer}{ext}/sae_weights.safetensors\", device=\"cuda\")\n",
    "        for i, t_layer in enumerate(model.transformer.h):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                else:\n",
    "                    submodule = getattr(t_layer, component)\n",
    "                break\n",
    "    else:\n",
    "        sae = AutoEncoder.from_pretrained(\n",
    "                f'dictionaries/pythia-70m-deduped/{component}_out_layer{layer}/10_32768/ae.pt',\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "        for i, t_layer in enumerate(model.gpt_neox.layers):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                elif component == 'attn':\n",
    "                    submodule = t_layer.attention\n",
    "                else:\n",
    "                    submodule = t_layer.mlp\n",
    "                break\n",
    "\n",
    "    feat_size = sae.encoder.weight.shape[0]\n",
    "\n",
    "    with model.trace(\"_\"):\n",
    "        output_submod = submodule.output.save()\n",
    "    is_tuple = isinstance(output_submod.value, tuple)\n",
    "\n",
    "    total_valid = 0\n",
    "    n_bins = 200\n",
    "    min_power = -10\n",
    "    max_power = 6\n",
    "    hist = t.zeros(n_bins).to('cuda')\n",
    "    nnz_hist = t.zeros(n_bins).to('cuda')  # will range from log(1) to log(feat_size)\n",
    "\n",
    "    for i in trange(0, 1000, batch_size):\n",
    "        entries = dset[i:i+batch_size]\n",
    "        valid_entries = []\n",
    "        for e in entries:\n",
    "            encoded = model.tokenizer(e, return_tensors='pt', max_length=seq_len, truncation=True).to('cuda')['input_ids']\n",
    "            if encoded.shape[1] == seq_len:\n",
    "                valid_entries.append(encoded)\n",
    "        if len(valid_entries) == 0:\n",
    "            continue\n",
    "        batch = t.cat(valid_entries, dim=0)\n",
    "        if simulate:\n",
    "            total_valid += len(valid_entries)\n",
    "            continue\n",
    "\n",
    "        with model.trace(batch), t.no_grad():\n",
    "            x = submodule.output\n",
    "            if is_tuple:\n",
    "                x = x[0]\n",
    "            f = sae.encode(x).save()\n",
    "\n",
    "        if f.ndim == 2:\n",
    "            f = f.unsqueeze(0)\n",
    "\n",
    "        f_late = f[:, seq_len//2:, :]\n",
    "        nnz = (f_late != 0).sum(dim=2).flatten()   # [N, seq_len//2].flatten()\n",
    "        abs_f = abs(f_late)\n",
    "        nnz_hist += t.histc(t.log10(nnz), bins=n_bins, min=np.log10(1), max=np.log10(feat_size))\n",
    "        hist += t.histc(t.log10(abs_f[abs_f != 0]), bins=n_bins, min=min_power, max=max_power)\n",
    "\n",
    "    hist = hist.cpu().numpy()\n",
    "\n",
    "    return hist, nnz_hist.cpu().numpy(), feat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'distribs-64-nnz-log10.pkl'\n",
    "import pickle\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dset = load_dataset(\"NeelNanda/pile-10k\")['train']['text']\n",
    "\n",
    "for model in ['gpt2', 'EleutherAI/pythia-70m-deduped']:\n",
    "    for component in ['attn', 'resid', 'mlp']:\n",
    "        for layer in range(12 if model == 'gpt2' else 6):\n",
    "            if (model, component, layer) in results:\n",
    "                continue\n",
    "            print(model, component, layer)\n",
    "            results[model, component, layer] = get_activation_stats(model, layer, component, dset, batch_size=16)\n",
    "            t.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results for GPT2, do a 12 x 3 plot where column 1 is resid, column 2 is attn, column 3 is mlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_model_hists(model_str, n_layers, results, hist_or_nnz='hist', thresh=None, as_sparsity=False):\n",
    "    fig, axs = plt.subplots(n_layers, 3, figsize=(15, 3.6*n_layers))\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        for i, component in enumerate(['resid', 'attn', 'mlp']):\n",
    "            if hist_or_nnz == 'hist':\n",
    "                min_val = -10\n",
    "                max_val = 6\n",
    "                xlabel = 'log10(Activation magnitude)'\n",
    "                hist = results[model_str, component, layer][0]\n",
    "                bins = np.linspace(min_val, max_val, 200)\n",
    "                if thresh is not None:\n",
    "                    if not as_sparsity:\n",
    "                        thresh_loc = np.searchsorted(bins, np.log(thresh))\n",
    "                    else:\n",
    "                        percentile_hist = np.cumsum(hist) / hist.sum()\n",
    "                        thresh_loc = np.searchsorted(percentile_hist, 1-thresh)\n",
    "                    hist = hist.copy()\n",
    "                    hist[:thresh_loc-1] = 0\n",
    "            else:\n",
    "                if thresh is not None:\n",
    "                    raise ValueError(\"thresh can only be computed for hist\")\n",
    "                min_val = 0\n",
    "                xlabel = 'NNZ'\n",
    "                hist = results[model_str, component, layer][1]\n",
    "                max_val = np.log10(results[model_str, component, layer][2])\n",
    "                bins = 10 ** (np.linspace(min_val, max_val, 200))\n",
    "                max_index = np.nonzero(hist)[0].max()\n",
    "                max_val = bins[max_index]\n",
    "                bins = bins[:max_index+1]\n",
    "                hist = hist[:max_index+1]\n",
    "\n",
    "            value_hist_color = 'blue'\n",
    "            ax = axs[layer, i]\n",
    "            ax.set_xlabel(xlabel, color=value_hist_color)\n",
    "            ax.set_ylabel('Frequency', color=value_hist_color)\n",
    "            ax.plot(bins, hist, color=value_hist_color)\n",
    "            ax.tick_params(axis='x', colors=value_hist_color)\n",
    "            ax.tick_params(axis='y', colors=value_hist_color)\n",
    "            # ax.set_xlim(min(min_nnz, min_val), max(max_nnz, max_val))\n",
    "            # compute median value of activations\n",
    "            median_idx = (hist.cumsum() >= hist.sum() / 2).nonzero()[0][0]\n",
    "            median_val = bins[median_idx]\n",
    "            # compute variance of activations\n",
    "            total = hist.sum()\n",
    "            mean = (bins * hist).sum() / total\n",
    "            std = np.sqrt(((bins - mean)**2 * hist).sum() / total)\n",
    "            ax.set_title(f'{component} layer {layer} (log(total) = {np.log(total):.2f})')\n",
    "            # vertical line at mean\n",
    "            ax.axvline(median_val, color='r', linestyle='--')\n",
    "            # add text with mean\n",
    "            ax.text(median_val+0.5, hist.max(), f'{median_val:.2f} +- {std:.2f}', color='r')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'hist', thresh=8e-6, as_sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2.71828 ** 15) / (64 * 958)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the edges based on the visual structure from the provided image\n",
    "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "edges = {   # key corresponds to height of node\n",
    "    10: [(\"A\", \"B\"), (\"A\", \"C\")],\n",
    "    9: [(\"B\", \"D\"), (\"B\", \"E\")],\n",
    "    8: [(\"C\", \"F\"), (\"I\", \"H\")],\n",
    "    4: [(\"G\", \"J\")],\n",
    "    7: [(\"F\", \"G\"), (\"H\", \"G\")],\n",
    "    6: [(\"T\", \"U\"), (\"T\", \"R\"), (\"S\", \"R\"), (\"J\", \"K\"), (\"J\", \"L\")],\n",
    "    5: [(\"U\", \"V\"), (\"U\", \"W\"), (\"K\", \"M\"), (\"K\", \"N\"), (\"L\", \"O\"), (\"L\", \"P\")]\n",
    "}\n",
    "all_edges = []\n",
    "for v in edges.values():\n",
    "    all_edges.extend(v)\n",
    "G.add_edges_from(all_edges)\n",
    "G.add_node(\"Q\")\n",
    "\n",
    "positions = {\n",
    "    \"A\": (0, 10),\n",
    "    \"B\": (-1, 9),\n",
    "    \"C\": (1, 9),\n",
    "    \"D\": (-2, 8),\n",
    "    \"E\": (-1, 8),\n",
    "    \"F\": (0, 8),\n",
    "    \"G\": (1, 7),\n",
    "    \"H\": (2, 8),\n",
    "    \"I\": (3, 9),\n",
    "    \"J\": (1, 6),\n",
    "    \"K\": (0, 5),\n",
    "    \"L\": (3, 5),\n",
    "    \"M\": (-1, 4),\n",
    "    \"N\": (0, 4),\n",
    "    \"O\": (2, 4),\n",
    "    \"P\": (4, 4),\n",
    "    \"Q\": (-4, 7),\n",
    "    \"R\": (-2, 6),\n",
    "    \"S\": (-1, 7),\n",
    "    \"T\": (-2, 7),\n",
    "    \"U\": (-3, 6),\n",
    "    \"V\": (-5, 5),\n",
    "    \"W\": (-4, 5),\n",
    "}\n",
    "\n",
    "\n",
    "nx.draw(G, pos=positions, with_labels=True, node_size=200, node_color='skyblue', font_size=10, font_weight='bold', font_color='black', edge_color='black', width=2, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "\n",
    "# %%\n",
    "# Filter graph functionality.\n",
    "def filter_graph(\n",
    "    graph, leaf_nodes: list | None = None\n",
    "):\n",
    "    \"\"\"Filter a directed graph down to a source-to-sink subgraph.\"\"\"\n",
    "\n",
    "    # `leaf_nodes` starts off with all nodes without outgoing edges.\n",
    "    if leaf_nodes is None:\n",
    "        leaf_nodes = []\n",
    "\n",
    "        for node in graph.nodes:\n",
    "            if not list(graph.successors(node)):\n",
    "                leaf_nodes.append(node)\n",
    "    print(leaf_nodes)\n",
    "    # Prunes out all the non-final-layer leaf nodes and append upstream\n",
    "    # relevant nodes.\n",
    "    for node in copy(leaf_nodes):\n",
    "        if node != f\"A\":\n",
    "            upstream_nodes = list(graph.predecessors(node))\n",
    "            leaf_nodes.extend(upstream_nodes)\n",
    "\n",
    "            graph.remove_node(node)\n",
    "        leaf_nodes.remove(node)\n",
    "\n",
    "    # Recurses if necessary.\n",
    "    if leaf_nodes:\n",
    "        leaf_nodes: list = list(set(leaf_nodes))\n",
    "        graph = filter_graph(graph, leaf_nodes)\n",
    "\n",
    "    return graph\n",
    "prune_G = filter_graph(G)\n",
    "nx.draw(prune_G, pos=positions, with_labels=True, node_size=200, node_color='skyblue', font_size=10, font_weight='bold', font_color='black', edge_color='black', width=2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "for layer in range(12):\n",
    "    tens = hf_hub_download('jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs',\n",
    "                    f'v5_32k_layer_{layer}.pt/sae_weights.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "tensors = {}\n",
    "with safe_open(tens, 'pt') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors['W_enc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read from and write to the module-level interface.\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "def parse_slice(slice_string: str) -> slice:\n",
    "    \"\"\"Parse any valid slice string into its slice object.\"\"\"\n",
    "\n",
    "    start = stop = step = None\n",
    "    slice_parts: list = slice_string.split(\":\")\n",
    "\n",
    "    if not 0 <= len(slice_parts) <= 3:\n",
    "        raise ValueError(\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                Slice string {slice_string} is not well-formed.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Remember that Python evaluates empty strings as falsy.\n",
    "    if slice_parts[0]:\n",
    "        start = int(slice_parts[0])\n",
    "    if len(slice_parts) > 1 and slice_parts[1]:\n",
    "        stop = int(slice_parts[1])\n",
    "    if len(slice_parts) == 3 and slice_parts[2]:\n",
    "        step = int(slice_parts[2])\n",
    "\n",
    "    layers_slice = slice(start, stop, step)\n",
    "\n",
    "    if layers_slice.start is not None and layers_slice.stop is not None:\n",
    "        assert start < stop, dedent(\n",
    "            f\"\"\"\n",
    "            Slice start ({layers_slice.start}) must be less than stop\n",
    "            ({layers_slice.stop})\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return layers_slice\n",
    "\n",
    "\n",
    "def validate_slice(model: PreTrainedModel, layers_slice: slice) -> None:\n",
    "    \"\"\"\n",
    "    See whether the layers slice fits in the model's layers.\n",
    "\n",
    "    Note that this is unnecessary when the slice is preprocessed with\n",
    "    `slice_to_seq`; only use this when you need to validate the _slice_ object,\n",
    "    not the corresponding range.\n",
    "    \"\"\"\n",
    "\n",
    "    if layers_slice.stop is None:\n",
    "        return\n",
    "\n",
    "    # num_hidden_layers is not inclusive.\n",
    "    last_layer: int = model.config.num_hidden_layers - 1\n",
    "\n",
    "    # slice.stop is not inclusive.\n",
    "    if last_layer < layers_slice.stop - 1:\n",
    "        raise ValueError(\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                The layers slice {layers_slice} is out of bounds for the\n",
    "                model's layer count.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def sanitize_model_name(model_name: str) -> str:\n",
    "    \"\"\"Sanitize model names for saving and loading.\"\"\"\n",
    "\n",
    "    return model_name.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "def cache_layer_tensor(\n",
    "    layer_tensor: t.Tensor,\n",
    "    layer_idx: int,\n",
    "    save_append: str,\n",
    "    base_file: str,\n",
    "    model_name: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Cache per layer tensors in appropriate subdirectories.\n",
    "\n",
    "    Base file is `__file__` in the calling module. Save append should be _just_\n",
    "    the file name and extension, not any additional path. Model name will be\n",
    "    sanitized, so HF hub names are kosher.\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(\n",
    "        layer_idx, int\n",
    "    ), f\"Layer index {layer_idx} is not an int.\"\n",
    "    # Python bools are an int subclass.\n",
    "    assert not isinstance(\n",
    "        layer_idx, bool\n",
    "    ), f\"Layer index {layer_idx} is a bool, not an int.\"\n",
    "\n",
    "    save_dir_path: str = save_paths(base_file, \"\")\n",
    "    safe_model_name = sanitize_model_name(model_name)\n",
    "\n",
    "    # Subdirectory structure in the save directory is\n",
    "    # data/models/layers/tensor.pt.\n",
    "    save_subdir_path: str = save_dir_path + f\"/{safe_model_name}/{layer_idx}\"\n",
    "\n",
    "    os.makedirs(save_subdir_path, exist_ok=True)\n",
    "    t.save(layer_tensor, save_subdir_path + f\"/{save_append}\")\n",
    "\n",
    "\n",
    "def slice_to_range(model: PreTrainedModel, input_slice: slice) -> range:\n",
    "    \"\"\"Build a range corresponding to an input slice.\"\"\"\n",
    "\n",
    "    if input_slice.start is None:\n",
    "        start = 0\n",
    "    elif input_slice.start < 0:\n",
    "        start: int = model.config.num_hidden_layers + input_slice.start\n",
    "    else:\n",
    "        start: int = input_slice.start\n",
    "\n",
    "    if input_slice.stop is None:\n",
    "        stop = model.config.num_hidden_layers\n",
    "    elif input_slice.stop < 0:\n",
    "        stop: int = model.config.num_hidden_layers + input_slice.stop\n",
    "    else:\n",
    "        stop: int = input_slice.stop\n",
    "\n",
    "    step: int = 1 if input_slice.step is None else input_slice.step\n",
    "\n",
    "    # Truncate final ranges to the model's size.\n",
    "    output_range = range(\n",
    "        max(start, 0),\n",
    "        min(stop, model.config.num_hidden_layers),\n",
    "        step,\n",
    "    )\n",
    "\n",
    "    return output_range\n",
    "\n",
    "\n",
    "def load_input_token_ids(prompt_ids_path: str) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Load input ids.\n",
    "\n",
    "    These are constant across layers, making this a simpler job.\n",
    "    \"\"\"\n",
    "    prompts_ids: np.ndarray = np.load(prompt_ids_path, allow_pickle=True)\n",
    "    prompts_ids_list = prompts_ids.tolist()\n",
    "    unpacked_ids: list[list[int]] = [\n",
    "        elem for question_list in prompts_ids_list for elem in question_list\n",
    "    ]\n",
    "\n",
    "    return unpacked_ids\n",
    "\n",
    "\n",
    "def load_yaml_constants(base_file):\n",
    "    \"\"\"Load config files.\"\"\"\n",
    "\n",
    "    current_dir = Path(base_file).parent\n",
    "    hf_access_file: str = \"config/hf_access.yaml\"\n",
    "    central_config_file: str = \"config/central_config.yaml\"\n",
    "\n",
    "    if current_dir.name == \"sparse_coding\":\n",
    "        hf_access_path = current_dir / hf_access_file\n",
    "        central_config_path = current_dir / central_config_file\n",
    "\n",
    "    elif current_dir.name in (\"interp_tools\", \"rasp\"):\n",
    "        hf_access_path = current_dir.parent / hf_access_file\n",
    "        central_config_path = current_dir.parent / central_config_file\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                Trying to access config files from an unfamiliar working\n",
    "                directory: {current_dir}\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        with open(hf_access_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            access = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"hf_access.yaml not found. Creating it now.\")\n",
    "        with open(hf_access_path, \"w\", encoding=\"utf-8\") as w:\n",
    "            w.write('HF_ACCESS_TOKEN: \"\"\\n')\n",
    "        access = {}\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)\n",
    "\n",
    "    with open(central_config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            config = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(e)\n",
    "\n",
    "    return access, config\n",
    "\n",
    "\n",
    "def save_paths(base_file, save_append: str) -> str:\n",
    "    \"\"\"Route to save paths from the current working directory.\"\"\"\n",
    "\n",
    "    assert isinstance(\n",
    "        save_append, str\n",
    "    ), f\"`save_append` must be a string: {save_append}.\"\n",
    "\n",
    "    current_dir = Path(base_file).parent\n",
    "\n",
    "    save_path = current_dir / \"data\" / save_append\n",
    "    return str(save_path)\n",
    "\n",
    "\n",
    "\n",
    "def load_layer_tensors(\n",
    "    model_dir: str,\n",
    "    layer_idx: int,\n",
    "    encoder_file: str,\n",
    "    biases_file: str,\n",
    "    base_file: str,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    Return the autoencoder, bias tensors for a model layer.\n",
    "\n",
    "    `base_file should be __file__ in the calling module.\n",
    "    \"\"\"\n",
    "\n",
    "    encoder = t.load(\n",
    "        save_paths(\n",
    "            base_file,\n",
    "            (\n",
    "                sanitize_model_name(model_dir)\n",
    "                + \"/\"\n",
    "                + str(layer_idx)\n",
    "                + \"/\"\n",
    "                + encoder_file\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    bias = t.load(\n",
    "        save_paths(\n",
    "            base_file,\n",
    "            (\n",
    "                sanitize_model_name(model_dir)\n",
    "                + \"/\"\n",
    "                + str(layer_idx)\n",
    "                + \"/\"\n",
    "                + biases_file\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return encoder, bias\n",
    "\n",
    "\n",
    "def load_layer_feature_indices(\n",
    "    model_dir: str,\n",
    "    layer_idx: int,\n",
    "    top_k_info_file: str,\n",
    "    base_file: str,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Return the meaningful feature indices for a model layer.\n",
    "\n",
    "    `base_file` should be `__file__` in the calling module.\n",
    "    \"\"\"\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    with open(\n",
    "        save_paths(\n",
    "            base_file,\n",
    "            (\n",
    "                sanitize_model_name(model_dir)\n",
    "                + \"/\"\n",
    "                + str(layer_idx)\n",
    "                + \"/\"\n",
    "                + top_k_info_file\n",
    "            ),\n",
    "        ),\n",
    "        mode=\"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Skip the header.\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def load_layer_feature_labels(\n",
    "    model_dir: str,\n",
    "    layer_idx: int,\n",
    "    feature_idx: int,\n",
    "    top_k_info_file: str,\n",
    "    base_file: str,\n",
    ") -> tuple[list[str], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Return the top-k input token labels for an encoder layer feature.\n",
    "\n",
    "    `base_file` should be `__file__` in the calling module.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\n",
    "        save_paths(\n",
    "            base_file,\n",
    "            (\n",
    "                sanitize_model_name(model_dir)\n",
    "                + \"/\"\n",
    "                + str(layer_idx)\n",
    "                + \"/\"\n",
    "                + top_k_info_file\n",
    "            ),\n",
    "        ),\n",
    "        mode=\"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Skip the header.\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            if int(row[0]) == feature_idx:\n",
    "                context_ints = []\n",
    "                context_str = row[1]\n",
    "                context_sublists = context_str.split(\"], \")\n",
    "\n",
    "                for list_str in context_sublists:\n",
    "                    list_int = []\n",
    "                    list_str = list_str.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "                    list_str = list_str.split(\", \")\n",
    "                    for integer in list_str:\n",
    "                        integer = int(integer.strip(\"'\"))\n",
    "                        list_int.append(integer)\n",
    "                    context_ints.append(list_int)\n",
    "\n",
    "                act_floats = []\n",
    "                acts_str = row[-1]\n",
    "                acts_sublists = acts_str.split(\"], \")\n",
    "\n",
    "                for list_str in acts_sublists:\n",
    "                    list_flt = []\n",
    "                    list_str = list_str.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "                    list_str = list_str.split(\", \")\n",
    "                    for flt in list_str:\n",
    "                        flt = float(flt)\n",
    "                        list_flt.append(flt)\n",
    "                    act_floats.append(list_flt)\n",
    "\n",
    "                return (context_ints, act_floats)\n",
    "\n",
    "        raise ValueError(\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                Feature index {feature_idx} not found in layer {layer_idx}\n",
    "                autoencoder.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def pad_activations(\n",
    "    tensor: t.Tensor, max_length: int, accelerator\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Pad activation tensors to a given sequence length.\"\"\"\n",
    "\n",
    "    complement_length: int = max_length - tensor.size(1)\n",
    "    padding: t.Tensor = t.zeros(\n",
    "        tensor.size(0), complement_length, tensor.size(2)\n",
    "    ).to(tensor.device)\n",
    "    padding = accelerator.prepare(padding)\n",
    "    try:\n",
    "        return t.cat([tensor, padding], dim=1)\n",
    "    except RuntimeError:\n",
    "        gc.collect()\n",
    "        return t.cat([tensor, padding], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "def load_sublayer_autoencoder(\n",
    "    autoencoder_repo: str,\n",
    "    encoder_file: str,\n",
    "    enc_biases_file: str,\n",
    "    decoder_file: str,\n",
    "    dec_biases_file: str,\n",
    "    model_dir: str,\n",
    "    acts_layers_range: range,\n",
    "    base_file: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load sublayer autoencoders from HuggingFace.\n",
    "\n",
    "    The HF Hub interface for these is rather different, so they are getting\n",
    "    their own import function.\n",
    "    \"\"\"\n",
    "\n",
    "    filename: str = \"sae_weights.safetensors\"\n",
    "\n",
    "    for idx in acts_layers_range:\n",
    "        subfolder: str = f\"v5_128k_layer_{idx}\"\n",
    "        safe_model_name = sanitize_model_name(model_dir)\n",
    "\n",
    "        file_url = hf_hub_download(\n",
    "            repo_id=autoencoder_repo,\n",
    "            filename=filename,\n",
    "            subfolder=subfolder,\n",
    "        )\n",
    "\n",
    "        tensors_dict: dict = {}\n",
    "        with safe_open(file_url, \"pt\") as f:\n",
    "            for k in f.keys():\n",
    "                tensors_dict[k] = f.get_tensor(k)\n",
    "\n",
    "        encoder = tensors_dict[\"W_enc\"]\n",
    "        enc_biases = tensors_dict[\"b_enc\"]\n",
    "        decoder = tensors_dict[\"W_dec\"]\n",
    "        dec_biases = tensors_dict[\"b_dec\"]\n",
    "\n",
    "        t.save(\n",
    "            encoder,\n",
    "            save_paths(base_file, f\"{safe_model_name}/{idx}/{encoder_file}\"),\n",
    "        )\n",
    "        t.save(\n",
    "            enc_biases,\n",
    "            save_paths(\n",
    "                base_file, f\"{safe_model_name}/{idx}/{enc_biases_file}\"\n",
    "            ),\n",
    "        )\n",
    "        t.save(\n",
    "            decoder,\n",
    "            save_paths(base_file, f\"{safe_model_name}/{idx}/{decoder_file}\"),\n",
    "        )\n",
    "        t.save(\n",
    "            dec_biases,\n",
    "            save_paths(\n",
    "                base_file, f\"{safe_model_name}/{idx}/{dec_biases_file}\"\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTN_REPO: str = \"jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs\"\n",
    "ATTN_ENCODER_FILE = \"attn_encoder.pt\"\n",
    "ATTN_ENC_BIASES_FILE = \"attn_enc_biases.pt\"\n",
    "ATTN_DECODER_FILE = \"attn_decoder.pt\"\n",
    "ATTN_DEC_BIASES_FILE = \"attn_dec_biases.pt\"\n",
    "MODEL_DIR = \"openai-community/gpt2\"\n",
    "ACTS_LAYERS_RANGE = range(0, 12)\n",
    "\n",
    "load_sublayer_autoencoder(\n",
    "    ATTN_REPO,\n",
    "    ATTN_ENCODER_FILE,\n",
    "    ATTN_ENC_BIASES_FILE,\n",
    "    ATTN_DECODER_FILE,\n",
    "    ATTN_DEC_BIASES_FILE,\n",
    "    MODEL_DIR,\n",
    "    ACTS_LAYERS_RANGE,\n",
    "    '/home/jack/feature-circuits/dictionaries/dummy.py',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens\n",
    "t = sae_lens.SAE.from_pretrained(\"jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.random.manual_seed(1)\n",
    "x = torch.randn(2, 20, 300)\n",
    "numel_per_batch = x.shape[1] * x.shape[2]\n",
    "numel_per_batch_seq = x.shape[2]\n",
    "ind = x.flatten().topk(5).indices\n",
    "ind2 = torch.cat([ind // numel_per_batch, (ind % numel_per_batch) // numel_per_batch_seq, ind % numel_per_batch_seq], dim=0).reshape(3, -1).T\n",
    "ind = torch.stack(torch.unravel_index(ind, x.shape), dim=1)\n",
    "ind, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vjv_MR = torch.load('vjv_save_MR.pt')\n",
    "vjv_AR = torch.load('vjv_save_AR.pt')\n",
    "vjv_AM = torch.load('vjv_save_AM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of agg, with log scale\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_hist(ax, agg, title):\n",
    "    x = torch.cat(agg, dim=0).flatten()\n",
    "    nz_only = x[x > 0]\n",
    "    ax.hist(nz_only.cpu().numpy(), bins=1000)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"{title} (max={nz_only.max()})\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "plot_hist(axs[0], vjv_MR, \"MR\")\n",
    "plot_hist(axs[1], vjv_AR, \"AR\")\n",
    "plot_hist(axs[2], vjv_AM, \"AM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleu_model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda', dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loading_utils\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from functools import partial\n",
    "\n",
    "saes = []\n",
    "for i in range(12):\n",
    "    saes.append(SAE.from_pretrained(\n",
    "        release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "        sae_id = f\"blocks.{i}.hook_resid_pre\", # won't always be a hook point\n",
    "        device = 'cuda'\n",
    "    ))  # returns SAE, config, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = []\n",
    "for i in range(12):\n",
    "    saes.append(SAE.from_pretrained(\n",
    "        release = \"gpt2-small-hook-z-kk\",\n",
    "        sae_id = f\"blocks.{i}.hook_z\", # won't always be a hook point\n",
    "        device = 'cuda'\n",
    "    ))  # returns SAE, config, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(12):\n",
    "    repo = \"jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs\"\n",
    "    filename = f\"v5_128k_layer_{i}/sparsity.safetensors\"\n",
    "\n",
    "    path = hf_hub_download(repo, filename)\n",
    "\n",
    "    tensor_dict = dict()\n",
    "\n",
    "    with safe_open(path, 'pt') as f:\n",
    "        for k in f.keys():\n",
    "            tensor_dict[k] = f.get_tensor(k)\n",
    "\n",
    "    # plot sparsity values\n",
    "    plt.title(f\"Sparsity histogram for layer {i}\")\n",
    "    plt.hist(tensor_dict['sparsity'].flatten().cpu().numpy(), bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_saes = []\n",
    "for i in range(12):\n",
    "    saes.append(SAE.from_pretrained(\n",
    "        release = \"gpt2-small-mlp-tm\",\n",
    "        sae_id = f\"blocks.{i}.hook_mlp_out\", # won't always be a hook point\n",
    "        device = 'cuda'\n",
    "    ))  # returns SAE, config, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saes[0][0].W_enc.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
