{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm, trange\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "import histogram_aggregator as ha\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node and Edge Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist = ha.HistAggregator()\n",
    "pyth_name = 'NeelNanda_pile-10k_dict10_node0.1_edge0.01_n9990_aggnone_threshTrue_methodig_prunefirst-layer-sink_modelEleutherAI_pythia-70m-deduped'\n",
    "pythia_hist.load(f'./circuits/{pyth_name}.hist.pt').cpu()\n",
    "gpt_hist = ha.HistAggregator()\n",
    "gpt_hist.load('./circuits/NeelNanda_pile-10k_dictgpt2_node8e-06_edge8e-06_n9990_aggnone_threshFalse_methodig_prunefirst-layer-sink_modelgpt2.hist.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist.plot(6, 'nodes', 'acts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist.plot(6, 'edges', 'acts', thresh=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'edges', 'acts', thresh=30, thresh_type=ha.ThresholdType.PEAK_MATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: np.log10(v) for k,v in gpt_hist.compute_node_thresholds(1e-5, ha.ThresholdType.PERCENTILE).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'nodes', 'acts')#, thresh=1e-5, thresh_type=ha.ThresholdType.PERCENTILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'edges', 'acts')#, thresh=1e-5, thresh_type=ha.ThresholdType.PERCENTILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claimed feature sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(12):\n",
    "    repo = \"jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs\"\n",
    "    filename = f\"v5_128k_layer_{i}/sparsity.safetensors\"\n",
    "\n",
    "    path = hf_hub_download(repo, filename)\n",
    "\n",
    "    tensor_dict = dict()\n",
    "\n",
    "    with safe_open(path, 'pt') as f:\n",
    "        for k in f.keys():\n",
    "            tensor_dict[k] = f.get_tensor(k)\n",
    "\n",
    "    # plot sparsity values\n",
    "    plt.title(f\"Sparsity histogram for layer {i}\")\n",
    "    plt.hist(tensor_dict['sparsity'].flatten().cpu().numpy(), bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old activation-based histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_stats(model_str, layer, component, dset, seq_len=64, batch_size=5, simulate=False):\n",
    "\n",
    "    model = LanguageModel(model_str, device_map='cuda', dispatch=True)\n",
    "    if model_str == 'gpt2':\n",
    "        # Load GPT2 model\n",
    "\n",
    "        # load SAE\n",
    "        if component == 'resid':\n",
    "            n_feats = '32k'\n",
    "            loc = 'post'\n",
    "            ext = '.pt'\n",
    "        else:\n",
    "            n_feats = '128k'\n",
    "            loc = 'out'\n",
    "            ext = ''\n",
    "\n",
    "\n",
    "        repo = f\"jbloom/GPT2-Small-OAI-v5-{n_feats}-{component}-{loc}-SAEs\"\n",
    "        sae = AutoEncoder.from_hf(repo, f\"v5_{n_feats}_layer_{layer}{ext}/sae_weights.safetensors\", device=\"cuda\")\n",
    "        for i, t_layer in enumerate(model.transformer.h):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                else:\n",
    "                    submodule = getattr(t_layer, component)\n",
    "                break\n",
    "    else:\n",
    "        sae = AutoEncoder.from_pretrained(\n",
    "                f'dictionaries/pythia-70m-deduped/{component}_out_layer{layer}/10_32768/ae.pt',\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "        for i, t_layer in enumerate(model.gpt_neox.layers):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                elif component == 'attn':\n",
    "                    submodule = t_layer.attention\n",
    "                else:\n",
    "                    submodule = t_layer.mlp\n",
    "                break\n",
    "\n",
    "    feat_size = sae.encoder.weight.shape[0]\n",
    "\n",
    "    with model.trace(\"_\"):\n",
    "        output_submod = submodule.output.save()\n",
    "    is_tuple = isinstance(output_submod.value, tuple)\n",
    "\n",
    "    total_valid = 0\n",
    "    n_bins = 200\n",
    "    min_power = -10\n",
    "    max_power = 6\n",
    "    hist = t.zeros(n_bins).to('cuda')\n",
    "    nnz_hist = t.zeros(n_bins).to('cuda')  # will range from log(1) to log(feat_size)\n",
    "\n",
    "    for i in trange(0, 1000, batch_size):\n",
    "        entries = dset[i:i+batch_size]\n",
    "        valid_entries = []\n",
    "        for e in entries:\n",
    "            encoded = model.tokenizer(e, return_tensors='pt', max_length=seq_len, truncation=True).to('cuda')['input_ids']\n",
    "            if encoded.shape[1] == seq_len:\n",
    "                valid_entries.append(encoded)\n",
    "        if len(valid_entries) == 0:\n",
    "            continue\n",
    "        batch = t.cat(valid_entries, dim=0)\n",
    "        if simulate:\n",
    "            total_valid += len(valid_entries)\n",
    "            continue\n",
    "\n",
    "        with model.trace(batch), t.no_grad():\n",
    "            x = submodule.output\n",
    "            if is_tuple:\n",
    "                x = x[0]\n",
    "            f = sae.encode(x).save()\n",
    "\n",
    "        if f.ndim == 2:\n",
    "            f = f.unsqueeze(0)\n",
    "\n",
    "        f_late = f[:, seq_len//2:, :]\n",
    "        nnz = (f_late != 0).sum(dim=2).flatten()   # [N, seq_len//2].flatten()\n",
    "        abs_f = abs(f_late)\n",
    "        nnz_hist += t.histc(t.log10(nnz), bins=n_bins, min=np.log10(1), max=np.log10(feat_size))\n",
    "        hist += t.histc(t.log10(abs_f[abs_f != 0]), bins=n_bins, min=min_power, max=max_power)\n",
    "\n",
    "    hist = hist.cpu().numpy()\n",
    "\n",
    "    return hist, nnz_hist.cpu().numpy(), feat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'distribs-64-nnz-log10.pkl'\n",
    "import pickle\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dset = load_dataset(\"NeelNanda/pile-10k\")['train']['text']\n",
    "\n",
    "for model in ['gpt2', 'EleutherAI/pythia-70m-deduped']:\n",
    "    for component in ['attn', 'resid', 'mlp']:\n",
    "        for layer in range(12 if model == 'gpt2' else 6):\n",
    "            if (model, component, layer) in results:\n",
    "                continue\n",
    "            print(model, component, layer)\n",
    "            results[model, component, layer] = get_activation_stats(model, layer, component, dset, batch_size=16)\n",
    "            t.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'hist', thresh=8e-6, as_sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'nnz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
