{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm, trange\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "from activation_utils import SparseAct\n",
    "from attribution import patching_effect, jvp\n",
    "from circuit_plotting import plot_circuit, plot_circuit_posaligned\n",
    "from loading_utils import load_examples, load_examples_nopair\n",
    "import histogram_aggregator as ha\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = t.load('./circuits/NeelNanda_pile-10k_dict10_node0.1_edge0.01_n9990_aggnone_threshTrue_methodig_prunefirst-layer-sink_modelEleutherAI_pythia-70m-deduped.hist.pt')\n",
    "dg = t.load('./circuits/NeelNanda_pile-10k_dictgpt2_node8e-06_edge8e-06_n9990_aggnone_threshFalse_methodig_prunefirst-layer-sink_modelgpt2.hist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_stats(model_str, layer, component, dset, seq_len=64, batch_size=5, simulate=False):\n",
    "\n",
    "    model = LanguageModel(model_str, device_map='cuda', dispatch=True)\n",
    "    if model_str == 'gpt2':\n",
    "        # Load GPT2 model\n",
    "\n",
    "        # load SAE\n",
    "        if component == 'resid':\n",
    "            n_feats = '32k'\n",
    "            loc = 'post'\n",
    "            ext = '.pt'\n",
    "        else:\n",
    "            n_feats = '128k'\n",
    "            loc = 'out'\n",
    "            ext = ''\n",
    "\n",
    "\n",
    "        repo = f\"jbloom/GPT2-Small-OAI-v5-{n_feats}-{component}-{loc}-SAEs\"\n",
    "        sae = AutoEncoder.from_hf(repo, f\"v5_{n_feats}_layer_{layer}{ext}/sae_weights.safetensors\", device=\"cuda\")\n",
    "        for i, t_layer in enumerate(model.transformer.h):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                else:\n",
    "                    submodule = getattr(t_layer, component)\n",
    "                break\n",
    "    else:\n",
    "        sae = AutoEncoder.from_pretrained(\n",
    "                f'dictionaries/pythia-70m-deduped/{component}_out_layer{layer}/10_32768/ae.pt',\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "        for i, t_layer in enumerate(model.gpt_neox.layers):\n",
    "            if i == layer:\n",
    "                if component == 'resid':\n",
    "                    submodule = t_layer\n",
    "                elif component == 'attn':\n",
    "                    submodule = t_layer.attention\n",
    "                else:\n",
    "                    submodule = t_layer.mlp\n",
    "                break\n",
    "\n",
    "    feat_size = sae.encoder.weight.shape[0]\n",
    "\n",
    "    with model.trace(\"_\"):\n",
    "        output_submod = submodule.output.save()\n",
    "    is_tuple = isinstance(output_submod.value, tuple)\n",
    "\n",
    "    total_valid = 0\n",
    "    n_bins = 200\n",
    "    min_power = -10\n",
    "    max_power = 6\n",
    "    hist = t.zeros(n_bins).to('cuda')\n",
    "    nnz_hist = t.zeros(n_bins).to('cuda')  # will range from log(1) to log(feat_size)\n",
    "\n",
    "    for i in trange(0, 1000, batch_size):\n",
    "        entries = dset[i:i+batch_size]\n",
    "        valid_entries = []\n",
    "        for e in entries:\n",
    "            encoded = model.tokenizer(e, return_tensors='pt', max_length=seq_len, truncation=True).to('cuda')['input_ids']\n",
    "            if encoded.shape[1] == seq_len:\n",
    "                valid_entries.append(encoded)\n",
    "        if len(valid_entries) == 0:\n",
    "            continue\n",
    "        batch = t.cat(valid_entries, dim=0)\n",
    "        if simulate:\n",
    "            total_valid += len(valid_entries)\n",
    "            continue\n",
    "\n",
    "        with model.trace(batch), t.no_grad():\n",
    "            x = submodule.output\n",
    "            if is_tuple:\n",
    "                x = x[0]\n",
    "            f = sae.encode(x).save()\n",
    "\n",
    "        if f.ndim == 2:\n",
    "            f = f.unsqueeze(0)\n",
    "\n",
    "        f_late = f[:, seq_len//2:, :]\n",
    "        nnz = (f_late != 0).sum(dim=2).flatten()   # [N, seq_len//2].flatten()\n",
    "        abs_f = abs(f_late)\n",
    "        nnz_hist += t.histc(t.log10(nnz), bins=n_bins, min=np.log10(1), max=np.log10(feat_size))\n",
    "        hist += t.histc(t.log10(abs_f[abs_f != 0]), bins=n_bins, min=min_power, max=max_power)\n",
    "\n",
    "    hist = hist.cpu().numpy()\n",
    "\n",
    "    return hist, nnz_hist.cpu().numpy(), feat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'distribs-64-nnz-log10.pkl'\n",
    "import pickle\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dset = load_dataset(\"NeelNanda/pile-10k\")['train']['text']\n",
    "\n",
    "for model in ['gpt2', 'EleutherAI/pythia-70m-deduped']:\n",
    "    for component in ['attn', 'resid', 'mlp']:\n",
    "        for layer in range(12 if model == 'gpt2' else 6):\n",
    "            if (model, component, layer) in results:\n",
    "                continue\n",
    "            print(model, component, layer)\n",
    "            results[model, component, layer] = get_activation_stats(model, layer, component, dset, batch_size=16)\n",
    "            t.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results for GPT2, do a 12 x 3 plot where column 1 is resid, column 2 is attn, column 3 is mlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_hist(hist, bins, ax, xlabel, title):\n",
    "    value_hist_color = 'blue'\n",
    "    ax.set_xlabel(xlabel, color=value_hist_color)\n",
    "    ax.set_ylabel('Frequency', color=value_hist_color)\n",
    "    ax.plot(bins, hist, color=value_hist_color)\n",
    "    ax.tick_params(axis='x', colors=value_hist_color)\n",
    "    ax.tick_params(axis='y', colors=value_hist_color)\n",
    "    # ax.set_xlim(min(min_nnz, min_val), max(max_nnz, max_val))\n",
    "    # compute median value of activations\n",
    "    median_idx = (hist.cumsum() >= hist.sum() / 2).nonzero()[0][0]\n",
    "    median_val = bins[median_idx]\n",
    "    # compute variance of activations\n",
    "    total = hist.sum()\n",
    "    mean = (bins * hist).sum() / total\n",
    "    std = np.sqrt(((bins - mean)**2 * hist).sum() / total)\n",
    "    ax.set_title(f'{title} : (log10(total) = {np.log10(total):.2f})')\n",
    "    # vertical line at mean\n",
    "    ax.axvline(median_val, color='r', linestyle='--')\n",
    "    # add text with mean\n",
    "    ax.text(median_val+0.5, hist.max(), f'{median_val:.2f} +- {std:.2f}', color='r')\n",
    "\n",
    "def get_hist_settings(hist, n_feats, hist_or_nnz='hist', thresh=None, as_sparsity=False):\n",
    "    if hist_or_nnz == 'hist':\n",
    "        min_val = -10\n",
    "        max_val = 6\n",
    "        xlabel = 'log10(Activation magnitude)'\n",
    "        bins = np.linspace(min_val, max_val, 200)\n",
    "        if thresh is not None:\n",
    "            if not as_sparsity:\n",
    "                thresh_loc = np.searchsorted(bins, np.log(thresh))\n",
    "            else:\n",
    "                percentile_hist = np.cumsum(hist) / hist.sum()\n",
    "                thresh_loc = np.searchsorted(percentile_hist, 1-thresh)\n",
    "            hist = hist.copy()\n",
    "            hist[:thresh_loc-1] = 0\n",
    "    else:\n",
    "        if thresh is not None:\n",
    "            raise ValueError(\"thresh can only be computed for hist\")\n",
    "        min_val = 0\n",
    "        xlabel = 'NNZ'\n",
    "        max_val = np.log10(n_feats)\n",
    "        bins = 10 ** (np.linspace(min_val, max_val, 200))\n",
    "        max_index = np.nonzero(hist)[0].max()\n",
    "        max_val = bins[max_index]\n",
    "        bins = bins[:max_index+1]\n",
    "        hist = hist[:max_index+1]\n",
    "    return hist, bins, xlabel\n",
    "\n",
    "def get_hist_and_nfeats_activations(layer, component, model_str, hist_or_nnz, results):\n",
    "    if hist_or_nnz == 'hist':\n",
    "        hist = results[model_str, component, layer][0]\n",
    "    else:\n",
    "        hist = results[model_str, component, layer][1]\n",
    "    feat_size = results[model_str, component, layer][2]\n",
    "    return hist, feat_size\n",
    "\n",
    "def get_hist_for_node_effect(layer, component, model_str, hist_or_nnz, results):\n",
    "    hist_type = 'acts' if hist_or_nnz == 'hist' else 'nnz'\n",
    "    key = f'node_{hist_type}'\n",
    "    if model_str == 'gpt2':\n",
    "        mod_name = f'.transformer.h.{layer}'\n",
    "    else:\n",
    "        mod_name = f'.gpt_neox.layers.{layer}'\n",
    "\n",
    "    match component:\n",
    "        case 'resid':\n",
    "            mod_name += ''\n",
    "        case 'attn':\n",
    "            mod_name += '.attention'\n",
    "        case 'mlp':\n",
    "            mod_name += '.mlp'\n",
    "    hist = results[key][mod_name]\n",
    "    if model_str == 'gpt2':\n",
    "        feat_size = 32768 if component == 'resid' else 131072\n",
    "\n",
    "    feat_size = results[model_str, component, layer][2]\n",
    "    return hist, feat_size\n",
    "\n",
    "\n",
    "def plot_model_hists(model_str, n_layers, hist_getter, hist_or_nnz='hist', thresh=None, as_sparsity=False):\n",
    "    fig, axs = plt.subplots(n_layers, 3, figsize=(15, 3.6*n_layers))\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        for i, component in enumerate(['resid', 'attn', 'mlp']):\n",
    "            hist, feat_size = hist_getter(layer, component, model_str, hist_or_nnz)\n",
    "            hist, bins, xlabel = get_hist_settings(hist, feat_size, hist_or_nnz, thresh, as_sparsity)\n",
    "            plot_hist(hist, bins, axs[layer, i], xlabel, f'{model_str} {component} layer {layer}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_edge_hists(model_str, result_dict, hist_or_nnz, thresh=None, as_sparsity=False):\n",
    "    hist_type = 'acts' if hist_or_nnz == 'hist' else 'nnz'\n",
    "    key = f'edge_{hist_type}'\n",
    "    results = result_dict[key]\n",
    "    n_edges = len(results)\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(n_edges / n_cols)\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 3.6*n_rows))\n",
    "\n",
    "    for edge_name, hist in results.items():\n",
    "        hist, bins, xlabel = get_hist_settings(hist, feat_size, hist_or_nnz, thresh, as_sparsity)\n",
    "        plot_hist(hist, bins, axs[layer, component], xlabel, f'{model_str} {component} layer {layer}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.node_nnz = data['node_nnz']\n",
    "        # self.node_acts = data['node_acts']\n",
    "        # self.edge_nnz = data['edge_nnz']\n",
    "        # self.edge_acts = data['edge_acts']\n",
    "        # self.nnz_max = data['nnz_max']\n",
    "        # self.act_min, self.act_max = data['act_min_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(d):\n",
    "    d_new = {}\n",
    "    for k, v in d.items():\n",
    "        for path, hist in v.items():\n",
    "            if k not in d_new:\n",
    "                d_new[k] = {}\n",
    "            if isinstance(path, tuple):\n",
    "                up, down = ha.normalize_path(path[0]), ha.normalize_path(path[1])\n",
    "                if up not in d_new[k]:\n",
    "                    d_new[k][up] = {}\n",
    "                d_new[k][up][down] = hist\n",
    "            else:\n",
    "                d_new[k][ha.normalize_path(path)] = hist\n",
    "    return d_new\n",
    "\n",
    "\n",
    "def get_n_feats(model_str, component):\n",
    "    if model_str == 'gpt2':\n",
    "        return 32768 if component == 'resid' else 131072\n",
    "    return 32768\n",
    "\n",
    "def build_nnz_max(d_new, model_str):\n",
    "    d_new['nnz_max'] = {}\n",
    "    for node in d_new['node_acts']:\n",
    "        comp = node.split('_')[0]\n",
    "        n_feats = get_n_feats(model_str, comp)\n",
    "        d_new['nnz_max'][node] = np.log10(n_feats)\n",
    "    d_new['act_min_max'] = (-10, 10)\n",
    "    d_new['model_str'] = model_str\n",
    "\n",
    "dg_new = update_dict(dg)\n",
    "dp_new = update_dict(dp)\n",
    "\n",
    "build_nnz_max(dg_new, 'gpt2')\n",
    "build_nnz_max(dp_new, 'EleutherAI/pythia-70m-deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist = ha.HistAggregator()\n",
    "gpt_hist.load(dg_new)\n",
    "pythia_hist = ha.HistAggregator()\n",
    "pythia_hist.load(dp_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist.node_acts['resid_0'][720:780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist.plot(6, 'EleutherAI/pythia-70m-deduped', 'nodes', 'acts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**(np.linspace(-10, 10, 1500)[750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(gpt_hist.node_acts, key=lambda x: gpt_hist.node_acts[x].nonzero()[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-10, 10, 1500)[750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'nodes', 'acts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'edges', 'acts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_hist.plot(6, 'edges', 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pythia_hist = ha.HistAggregator()\n",
    "new_pythia_hist.load('./circuits/NeelNanda_pile-10k_dict10_node0.1_edge0.01_n9990_aggnone_threshTrue_methodig_prunefirst-layer-sink_modelEleutherAI_pythia-70m-deduped.hist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist = ha.HistAggregator()\n",
    "gpt_hist.load('./circuits/NeelNanda_pile-10k_dictgpt2_node8e-06_edge8e-06_n9990_aggnone_threshFalse_methodig_prunefirst-layer-sink_modelgpt2.hist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hist.plot(12, 'nodes', 'acts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pythia_hist.plot(6, 'nodes', 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'hist', thresh=8e-6, as_sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('gpt2', 12, results, 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_hists('EleutherAI/pythia-70m-deduped', 6, results, 'nnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(12):\n",
    "    repo = \"jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs\"\n",
    "    filename = f\"v5_128k_layer_{i}/sparsity.safetensors\"\n",
    "\n",
    "    path = hf_hub_download(repo, filename)\n",
    "\n",
    "    tensor_dict = dict()\n",
    "\n",
    "    with safe_open(path, 'pt') as f:\n",
    "        for k in f.keys():\n",
    "            tensor_dict[k] = f.get_tensor(k)\n",
    "\n",
    "    # plot sparsity values\n",
    "    plt.title(f\"Sparsity histogram for layer {i}\")\n",
    "    plt.hist(tensor_dict['sparsity'].flatten().cpu().numpy(), bins=100)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
